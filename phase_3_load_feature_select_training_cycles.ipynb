{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324bf0d6",
   "metadata": {},
   "source": [
    "## **3.1 Load Features & Select Training Cycles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018cd799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 directories created/verified:\n",
      "  Plots: result/phase_3\\plot\n",
      "  Data:  result/phase_3\\data\n",
      "Random seed set to 42\n",
      "IEEE plot settings applied.\n",
      "\n",
      "Loaded features from: result/phase_2/data/features.csv\n",
      "  Shape: (1241, 9)\n",
      "  Total cycles: 1241\n",
      "\n",
      "=== Training Configuration ===\n",
      "Training cycles: 1 to 20 (inclusive)\n",
      "Training samples: 20\n",
      "Test samples: 1221\n",
      "\n",
      "Training subset selected\n",
      "  Cycle range: [1, 20]\n",
      "  Number of samples: 20\n",
      "\n",
      "Feature columns (n=8): ['capacity_Ah', 'energy_Wh', 'duration_s', 'v_min', 'v_max', 'v_mean', 'i_rms', 'dVdt_abs_mean']\n",
      "\n",
      "=== Training Data Summary ===\n",
      "       cycle_idx  capacity_Ah  energy_Wh  duration_s      v_min      v_max  \\\n",
      "count   20.00000    20.000000  20.000000    20.00000  20.000000  20.000000   \n",
      "mean    10.50000     3.248586  11.613322  3655.70000   2.999985   4.083179   \n",
      "std      5.91608     0.011178   0.043639    12.57441   0.000046   0.000646   \n",
      "min      1.00000     3.234622  11.556919  3640.00000   2.999850   4.081560   \n",
      "25%      5.75000     3.239967  11.579369  3646.00000   3.000000   4.083050   \n",
      "50%     10.50000     3.246181  11.604447  3653.00000   3.000000   4.083240   \n",
      "75%     15.25000     3.253749  11.635044  3661.50000   3.000000   4.083700   \n",
      "max     20.00000     3.270184  11.696697  3680.00000   3.000000   4.083850   \n",
      "\n",
      "          v_mean      i_rms  dVdt_abs_mean  \n",
      "count  20.000000  20.000000      20.000000  \n",
      "mean    3.574754   3.199526       0.000363  \n",
      "std     0.001201   0.000007       0.000002  \n",
      "min     3.572743   3.199514       0.000359  \n",
      "25%     3.573824   3.199521       0.000361  \n",
      "50%     3.574794   3.199528       0.000363  \n",
      "75%     3.575761   3.199531       0.000364  \n",
      "max     3.576643   3.199535       0.000366  \n"
     ]
    }
   ],
   "source": [
    "# === 1. Import Libraries ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.quantum_info import Statevector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === 2. Define Project Paths ===\n",
    "PHASE_NUMBER = 3\n",
    "\n",
    "RESULT_DIR = f\"result/phase_{PHASE_NUMBER}\"\n",
    "PLOT_DIR = os.path.join(RESULT_DIR, \"plot\")\n",
    "DATA_DIR = os.path.join(RESULT_DIR, \"data\")\n",
    "\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Phase {PHASE_NUMBER} directories created/verified:\")\n",
    "print(f\"  Plots: {PLOT_DIR}\")\n",
    "print(f\"  Data:  {DATA_DIR}\")\n",
    "\n",
    "# === 3. Set Random Seed for Reproducibility ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "print(f\"Random seed set to {SEED}\")\n",
    "\n",
    "# === 4. Matplotlib Plotting Settings (for IEEE) ===\n",
    "def setup_ieee_plots():\n",
    "    \"\"\"Apply consistent, professional plot settings for IEEE publication.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (8, 5),\n",
    "        'figure.dpi': 300,\n",
    "        'font.family': 'serif',\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'lines.linewidth': 2,\n",
    "        'lines.markersize': 5,\n",
    "        'grid.alpha': 0.3,\n",
    "        'grid.linestyle': '--',\n",
    "        'axes.grid': True,\n",
    "    })\n",
    "    print(\"IEEE plot settings applied.\")\n",
    "\n",
    "setup_ieee_plots()\n",
    "\n",
    "# === 5. Load Features & Select Training Cycles ===\n",
    "features_df = pd.read_csv('result/phase_2/data/features.csv')\n",
    "print(f\"\\nLoaded features from: result/phase_2/data/features.csv\")\n",
    "print(f\"  Shape: {features_df.shape}\")\n",
    "print(f\"  Total cycles: {len(features_df)}\")\n",
    "\n",
    "N = 20\n",
    "print(f\"\\n=== Training Configuration ===\")\n",
    "print(f\"Training cycles: 1 to {N} (inclusive)\")\n",
    "print(f\"Training samples: {N}\")\n",
    "print(f\"Test samples: {len(features_df) - N}\")\n",
    "\n",
    "train_mask = (features_df['cycle_idx'] >= 1) & (features_df['cycle_idx'] <= N)\n",
    "train_df = features_df[train_mask].copy()\n",
    "print(f\"\\nTraining subset selected\")\n",
    "print(f\"  Cycle range: [{train_df['cycle_idx'].min()}, {train_df['cycle_idx'].max()}]\")\n",
    "print(f\"  Number of samples: {len(train_df)}\")\n",
    "\n",
    "feature_cols = ['capacity_Ah', 'energy_Wh', 'duration_s', \n",
    "                'v_min', 'v_max', 'v_mean', 'i_rms', 'dVdt_abs_mean']\n",
    "print(f\"\\nFeature columns (n={len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "print(f\"\\n=== Training Data Summary ===\")\n",
    "print(train_df[['cycle_idx'] + feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b679fa",
   "metadata": {},
   "source": [
    "## **3.2 Scale Features to [0, π] Using Training Min–Max (Persist Scaler)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7eb6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Scaling to [0, π] ===\n",
      "Training feature matrix shape: (20, 8)\n",
      "Full feature matrix shape: (1241, 8)\n",
      "\n",
      "--- Training Set Min/Max (per feature) ---\n",
      "capacity_Ah         : [3.234622, 3.270184]\n",
      "energy_Wh           : [11.556919, 11.696697]\n",
      "duration_s          : [3640.000000, 3680.000000]\n",
      "v_min               : [2.999850, 3.000000]\n",
      "v_max               : [4.081560, 4.083850]\n",
      "v_mean              : [3.572743, 3.576643]\n",
      "i_rms               : [3.199514, 3.199535]\n",
      "dVdt_abs_mean       : [0.000359, 0.000366]\n",
      "\n",
      "Features scaled to [0, π]\n",
      "  Training scaled shape: (20, 8)\n",
      "  Full scaled shape: (1241, 8)\n",
      "\n",
      "--- Scaled Training Data Range Check ---\n",
      "  Min values: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  Max values: [3.14159265 3.14159265 3.14159265 3.14159265 3.14159265 3.14159265\n",
      " 3.14159265 3.14159265]\n",
      "  Expected: all values in [0, 3.141593]\n",
      "\n",
      "Scaler saved to: result/phase_3\\data\\scaler.pkl\n",
      "Scaling parameters saved to: result/phase_3\\data\\scaling_params.json\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df[feature_cols].values\n",
    "X_full = features_df[feature_cols].values\n",
    "\n",
    "print(f\"=== Feature Scaling to [0, π] ===\")\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Full feature matrix shape: {X_full.shape}\")\n",
    "\n",
    "train_min = X_train.min(axis=0)\n",
    "train_max = X_train.max(axis=0)\n",
    "\n",
    "print(f\"\\n--- Training Set Min/Max (per feature) ---\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"{col:20s}: [{train_min[i]:.6f}, {train_max[i]:.6f}]\")\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_full_scaled = scaler.transform(X_full)\n",
    "\n",
    "print(f\"\\nFeatures scaled to [0, π]\")\n",
    "print(f\"  Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Full scaled shape: {X_full_scaled.shape}\")\n",
    "\n",
    "print(f\"\\n--- Scaled Training Data Range Check ---\")\n",
    "print(f\"  Min values: {X_train_scaled.min(axis=0)}\")\n",
    "print(f\"  Max values: {X_train_scaled.max(axis=0)}\")\n",
    "print(f\"  Expected: all values in [0, {np.pi:.6f}]\")\n",
    "\n",
    "scaler_path = os.path.join(DATA_DIR, 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\nScaler saved to: {scaler_path}\")\n",
    "\n",
    "scaling_params = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'train_min': train_min.tolist(),\n",
    "    'train_max': train_max.tolist(),\n",
    "    'target_range': [0, np.pi],\n",
    "    'n_features': len(feature_cols)\n",
    "}\n",
    "scaling_params_path = os.path.join(DATA_DIR, 'scaling_params.json')\n",
    "with open(scaling_params_path, 'w') as f:\n",
    "    json.dump(scaling_params, f, indent=2)\n",
    "print(f\"Scaling parameters saved to: {scaling_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dfd0a",
   "metadata": {},
   "source": [
    "## **3.3 Build Quantum Feature Map (ZZ/Pauli, Depth 1–2, 8 Qubits) & Kernel (Precomputed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4478b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quantum Feature Map Construction ===\n",
      "Number of qubits: 8\n",
      "Entangling depth: 2\n",
      "Feature encoding: ZZ/Pauli (RZ + RY + ZZ gates)\n",
      "\n",
      "Quantum circuit created\n",
      "  Circuit depth: 52\n",
      "  Number of gates: 80\n",
      "  Parameters: 8\n",
      "\n",
      "=== Computing Quantum Kernel Matrices ===\n",
      "This may take several minutes...\n",
      "\n",
      "--- Training kernel K_train (20 × 20) ---\n",
      "  Progress: 20/20 rows computed\n",
      "\n",
      "--- Full kernel K_full (1241 × 20) for scoring ---\n",
      "  Progress: 50/1241 rows computed\n",
      "  Progress: 100/1241 rows computed\n",
      "  Progress: 150/1241 rows computed\n",
      "  Progress: 200/1241 rows computed\n",
      "  Progress: 250/1241 rows computed\n",
      "  Progress: 300/1241 rows computed\n",
      "  Progress: 350/1241 rows computed\n",
      "  Progress: 400/1241 rows computed\n",
      "  Progress: 450/1241 rows computed\n",
      "  Progress: 500/1241 rows computed\n",
      "  Progress: 550/1241 rows computed\n",
      "  Progress: 600/1241 rows computed\n",
      "  Progress: 650/1241 rows computed\n",
      "  Progress: 700/1241 rows computed\n",
      "  Progress: 750/1241 rows computed\n",
      "  Progress: 800/1241 rows computed\n",
      "  Progress: 850/1241 rows computed\n",
      "  Progress: 900/1241 rows computed\n",
      "  Progress: 950/1241 rows computed\n",
      "  Progress: 1000/1241 rows computed\n",
      "  Progress: 1050/1241 rows computed\n",
      "  Progress: 1100/1241 rows computed\n",
      "  Progress: 1150/1241 rows computed\n",
      "  Progress: 1200/1241 rows computed\n",
      "  Progress: 1241/1241 rows computed\n",
      "\n",
      "Quantum kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "  K_train diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "\n",
      "Kernel matrices saved:\n",
      "  result/phase_3\\data\\K_quantum_train.npy\n",
      "  result/phase_3\\data\\K_quantum_full.npy\n",
      "Quantum parameters saved to: result/phase_3\\data\\quantum_kernel_params.json\n"
     ]
    }
   ],
   "source": [
    "def create_quantum_feature_map(n_qubits=8, depth=2):\n",
    "    \"\"\"\n",
    "    Create a ZZ/Pauli entangling feature map circuit.\n",
    "    \n",
    "    Args:\n",
    "        n_qubits: Number of qubits (must match feature dimension)\n",
    "        depth: Number of entangling layers\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit with parameter vector\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    params = ParameterVector('x', n_qubits)\n",
    "    \n",
    "    for d in range(depth):\n",
    "        for i in range(n_qubits):\n",
    "            qc.rz(params[i], i)\n",
    "            qc.ry(params[i], i)\n",
    "        \n",
    "        for i in range(n_qubits):\n",
    "            j = (i + 1) % n_qubits\n",
    "            qc.cx(i, j)\n",
    "            qc.rz(2 * (params[i] - params[j]), j)\n",
    "            qc.cx(i, j)\n",
    "    \n",
    "    return qc, params\n",
    "\n",
    "def quantum_kernel_element(x1, x2, feature_map, params):\n",
    "    \"\"\"\n",
    "    Compute quantum kernel element K(x1, x2) = |⟨φ(x1)|φ(x2)⟩|²\n",
    "    \"\"\"\n",
    "    qc1 = feature_map.assign_parameters({params[i]: x1[i] for i in range(len(x1))})\n",
    "    state1 = Statevector.from_instruction(qc1)\n",
    "    \n",
    "    qc2 = feature_map.assign_parameters({params[i]: x2[i] for i in range(len(x2))})\n",
    "    state2 = Statevector.from_instruction(qc2)\n",
    "    \n",
    "    overlap = np.abs(state1.inner(state2)) ** 2\n",
    "    return overlap\n",
    "\n",
    "def compute_quantum_kernel_matrix(X1, X2, feature_map, params):\n",
    "    \"\"\"\n",
    "    Compute full quantum kernel matrix K[i,j] = K(X1[i], X2[j])\n",
    "    \"\"\"\n",
    "    n1, n2 = len(X1), len(X2)\n",
    "    K = np.zeros((n1, n2))\n",
    "    \n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = quantum_kernel_element(X1[i], X2[j], feature_map, params)\n",
    "        \n",
    "        if (i + 1) % 50 == 0 or i == n1 - 1:\n",
    "            print(f\"  Progress: {i+1}/{n1} rows computed\")\n",
    "    \n",
    "    return K\n",
    "\n",
    "n_qubits = 8\n",
    "depth = 2\n",
    "print(f\"=== Quantum Feature Map Construction ===\")\n",
    "print(f\"Number of qubits: {n_qubits}\")\n",
    "print(f\"Entangling depth: {depth}\")\n",
    "print(f\"Feature encoding: ZZ/Pauli (RZ + RY + ZZ gates)\")\n",
    "\n",
    "feature_map, params = create_quantum_feature_map(n_qubits=n_qubits, depth=depth)\n",
    "print(f\"\\nQuantum circuit created\")\n",
    "print(f\"  Circuit depth: {feature_map.depth()}\")\n",
    "print(f\"  Number of gates: {len(feature_map.data)}\")\n",
    "print(f\"  Parameters: {len(params)}\")\n",
    "\n",
    "print(f\"\\n=== Computing Quantum Kernel Matrices ===\")\n",
    "print(f\"This may take several minutes...\")\n",
    "\n",
    "print(f\"\\n--- Training kernel K_train (20 × 20) ---\")\n",
    "K_quantum_train = compute_quantum_kernel_matrix(X_train_scaled, X_train_scaled, \n",
    "                                                 feature_map, params)\n",
    "\n",
    "print(f\"\\n--- Full kernel K_full ({len(X_full_scaled)} × 20) for scoring ---\")\n",
    "K_quantum_full = compute_quantum_kernel_matrix(X_full_scaled, X_train_scaled, \n",
    "                                                feature_map, params)\n",
    "\n",
    "print(f\"\\nQuantum kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_quantum_train.shape}\")\n",
    "print(f\"  K_full shape: {K_quantum_full.shape}\")\n",
    "print(f\"  K_train diagonal (should be ~1.0): {np.diag(K_quantum_train)[:5]}\")\n",
    "\n",
    "np.save(os.path.join(DATA_DIR, 'K_quantum_train.npy'), K_quantum_train)\n",
    "np.save(os.path.join(DATA_DIR, 'K_quantum_full.npy'), K_quantum_full)\n",
    "print(f\"\\nKernel matrices saved:\")\n",
    "print(f\"  {os.path.join(DATA_DIR, 'K_quantum_train.npy')}\")\n",
    "print(f\"  {os.path.join(DATA_DIR, 'K_quantum_full.npy')}\")\n",
    "\n",
    "quantum_params = {\n",
    "    'type': 'ZZ_Pauli_entangling',\n",
    "    'n_qubits': n_qubits,\n",
    "    'depth': depth,\n",
    "    'circuit_depth': feature_map.depth(),\n",
    "    'n_gates': len(feature_map.data),\n",
    "    'encoding': 'RZ + RY rotations with ZZ entanglement'\n",
    "}\n",
    "quantum_params_path = os.path.join(DATA_DIR, 'quantum_kernel_params.json')\n",
    "with open(quantum_params_path, 'w') as f:\n",
    "    json.dump(quantum_params, f, indent=2)\n",
    "print(f\"Quantum parameters saved to: {quantum_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8693fd",
   "metadata": {},
   "source": [
    "## **3.4 Prepare Baseline Kernels (RBF γ=median heuristic; Laplacian; Poly deg 2–3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592e0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RBF Kernel (Median Heuristic) ===\n",
      "Training pairwise distances:\n",
      "  Median distance: 3.472745\n",
      "  Mean distance: 3.537774\n",
      "  Std distance: 1.446672\n",
      "  Range: [0.766265, 6.955260]\n",
      "\n",
      "RBF gamma (median heuristic): 0.041460\n",
      "\n",
      "RBF kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "  K_train diagonal: [1. 1. 1. 1. 1.]\n",
      "\n",
      "=== Laplacian Kernel ===\n",
      "Using same gamma: 0.041460\n",
      "Laplacian kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "\n",
      "=== Polynomial Kernels ===\n",
      "Degree 2:\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "Degree 3:\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "\n",
      "=== Saving Baseline Kernel Matrices ===\n",
      "All baseline kernel matrices saved\n",
      "Baseline parameters saved to: result/phase_3\\data\\baseline_kernel_params.json\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel, laplacian_kernel, polynomial_kernel\n",
    "\n",
    "train_distances = pairwise_distances(X_train_scaled, metric='euclidean')\n",
    "train_dist_flat = train_distances[np.triu_indices_from(train_distances, k=1)]\n",
    "\n",
    "median_dist = np.median(train_dist_flat)\n",
    "gamma_rbf = 1.0 / (2 * median_dist ** 2)\n",
    "\n",
    "print(f\"=== RBF Kernel (Median Heuristic) ===\")\n",
    "print(f\"Training pairwise distances:\")\n",
    "print(f\"  Median distance: {median_dist:.6f}\")\n",
    "print(f\"  Mean distance: {train_dist_flat.mean():.6f}\")\n",
    "print(f\"  Std distance: {train_dist_flat.std():.6f}\")\n",
    "print(f\"  Range: [{train_dist_flat.min():.6f}, {train_dist_flat.max():.6f}]\")\n",
    "print(f\"\\nRBF gamma (median heuristic): {gamma_rbf:.6f}\")\n",
    "\n",
    "K_rbf_train = rbf_kernel(X_train_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "K_rbf_full = rbf_kernel(X_full_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "\n",
    "print(f\"\\nRBF kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_rbf_train.shape}\")\n",
    "print(f\"  K_full shape: {K_rbf_full.shape}\")\n",
    "print(f\"  K_train diagonal: {np.diag(K_rbf_train)[:5]}\")\n",
    "\n",
    "print(f\"\\n=== Laplacian Kernel ===\")\n",
    "print(f\"Using same gamma: {gamma_rbf:.6f}\")\n",
    "\n",
    "K_laplacian_train = laplacian_kernel(X_train_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "K_laplacian_full = laplacian_kernel(X_full_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "\n",
    "print(f\"Laplacian kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_laplacian_train.shape}\")\n",
    "print(f\"  K_full shape: {K_laplacian_full.shape}\")\n",
    "\n",
    "print(f\"\\n=== Polynomial Kernels ===\")\n",
    "\n",
    "K_poly2_train = polynomial_kernel(X_train_scaled, X_train_scaled, degree=2, coef0=1)\n",
    "K_poly2_full = polynomial_kernel(X_full_scaled, X_train_scaled, degree=2, coef0=1)\n",
    "\n",
    "print(f\"Degree 2:\")\n",
    "print(f\"  K_train shape: {K_poly2_train.shape}\")\n",
    "print(f\"  K_full shape: {K_poly2_full.shape}\")\n",
    "\n",
    "K_poly3_train = polynomial_kernel(X_train_scaled, X_train_scaled, degree=3, coef0=1)\n",
    "K_poly3_full = polynomial_kernel(X_full_scaled, X_train_scaled, degree=3, coef0=1)\n",
    "\n",
    "print(f\"Degree 3:\")\n",
    "print(f\"  K_train shape: {K_poly3_train.shape}\")\n",
    "print(f\"  K_full shape: {K_poly3_full.shape}\")\n",
    "\n",
    "print(f\"\\n=== Saving Baseline Kernel Matrices ===\")\n",
    "np.save(os.path.join(DATA_DIR, 'K_rbf_train.npy'), K_rbf_train)\n",
    "np.save(os.path.join(DATA_DIR, 'K_rbf_full.npy'), K_rbf_full)\n",
    "np.save(os.path.join(DATA_DIR, 'K_laplacian_train.npy'), K_laplacian_train)\n",
    "np.save(os.path.join(DATA_DIR, 'K_laplacian_full.npy'), K_laplacian_full)\n",
    "np.save(os.path.join(DATA_DIR, 'K_poly2_train.npy'), K_poly2_train)\n",
    "np.save(os.path.join(DATA_DIR, 'K_poly2_full.npy'), K_poly2_full)\n",
    "np.save(os.path.join(DATA_DIR, 'K_poly3_train.npy'), K_poly3_train)\n",
    "np.save(os.path.join(DATA_DIR, 'K_poly3_full.npy'), K_poly3_full)\n",
    "\n",
    "print(f\"All baseline kernel matrices saved\")\n",
    "\n",
    "baseline_params = {\n",
    "    'rbf': {\n",
    "        'type': 'radial_basis_function',\n",
    "        'gamma': gamma_rbf,\n",
    "        'gamma_method': 'median_heuristic',\n",
    "        'median_distance': median_dist\n",
    "    },\n",
    "    'laplacian': {\n",
    "        'type': 'laplacian',\n",
    "        'gamma': gamma_rbf,\n",
    "        'gamma_method': 'same_as_rbf'\n",
    "    },\n",
    "    'polynomial_deg2': {\n",
    "        'type': 'polynomial',\n",
    "        'degree': 2,\n",
    "        'coef0': 1\n",
    "    },\n",
    "    'polynomial_deg3': {\n",
    "        'type': 'polynomial',\n",
    "        'degree': 3,\n",
    "        'coef0': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "baseline_params_path = os.path.join(DATA_DIR, 'baseline_kernel_params.json')\n",
    "with open(baseline_params_path, 'w') as f:\n",
    "    json.dump(baseline_params, f, indent=2)\n",
    "print(f\"Baseline parameters saved to: {baseline_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884db278",
   "metadata": {},
   "source": [
    "## **3.5 Train ν-OCSVM for Quantum & Baselines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fedd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ν-OCSVM Models ===\n",
      "ν parameter: 0.05 (target ~5% training outliers)\n",
      "Training samples: 20\n",
      "\n",
      "--- Quantum Kernel ---\n",
      "Quantum OCSVM trained\n",
      "  Support vectors: [19]\n",
      "  Support vector indices: [0 1 2 3 4]... (showing first 5)\n",
      "\n",
      "--- RBF Kernel ---\n",
      "RBF OCSVM trained\n",
      "  Support vectors: [7]\n",
      "\n",
      "--- Laplacian Kernel ---\n",
      "Laplacian OCSVM trained\n",
      "  Support vectors: [9]\n",
      "\n",
      "--- Polynomial (degree 2) ---\n",
      "Poly2 OCSVM trained\n",
      "  Support vectors: [2]\n",
      "\n",
      "--- Polynomial (degree 3) ---\n",
      "Poly3 OCSVM trained\n",
      "  Support vectors: [2]\n",
      "\n",
      "=== Computing Training Scores ===\n",
      "\n",
      "QUANTUM:\n",
      "  Score range: [-0.000408, 0.004158]\n",
      "  Score mean: 0.000208\n",
      "  Score std: 0.000929\n",
      "\n",
      "RBF:\n",
      "  Score range: [-0.000320, 0.080326]\n",
      "  Score mean: 0.024584\n",
      "  Score std: 0.025870\n",
      "\n",
      "LAPLACIAN:\n",
      "  Score range: [-0.000428, 0.037609]\n",
      "  Score mean: 0.007758\n",
      "  Score std: 0.010630\n",
      "\n",
      "POLY2:\n",
      "  Score range: [0.000000, 6.688983]\n",
      "  Score mean: 2.490045\n",
      "  Score std: 1.748143\n",
      "\n",
      "POLY3:\n",
      "  Score range: [-0.000000, 34.703951]\n",
      "  Score mean: 12.005305\n",
      "  Score std: 8.940211\n",
      "\n",
      "=== Saving Models ===\n",
      "quantum: result/phase_3\\data\\ocsvm_quantum.pkl\n",
      "rbf: result/phase_3\\data\\ocsvm_rbf.pkl\n",
      "laplacian: result/phase_3\\data\\ocsvm_laplacian.pkl\n",
      "poly2: result/phase_3\\data\\ocsvm_poly2.pkl\n",
      "poly3: result/phase_3\\data\\ocsvm_poly3.pkl\n",
      "Training scores saved to: result/phase_3\\data\\train_scores.pkl\n",
      "\n",
      "All models trained and saved successfully\n"
     ]
    }
   ],
   "source": [
    "nu = 0.05\n",
    "\n",
    "print(f\"=== Training ν-OCSVM Models ===\")\n",
    "print(f\"ν parameter: {nu} (target ~5% training outliers)\")\n",
    "print(f\"Training samples: {K_quantum_train.shape[0]}\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "print(f\"\\n--- Quantum Kernel ---\")\n",
    "ocsvm_quantum = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_quantum.fit(K_quantum_train)\n",
    "models['quantum'] = ocsvm_quantum\n",
    "print(f\"Quantum OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_quantum.n_support_}\")\n",
    "print(f\"  Support vector indices: {ocsvm_quantum.support_[:5]}... (showing first 5)\")\n",
    "\n",
    "print(f\"\\n--- RBF Kernel ---\")\n",
    "ocsvm_rbf = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_rbf.fit(K_rbf_train)\n",
    "models['rbf'] = ocsvm_rbf\n",
    "print(f\"RBF OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_rbf.n_support_}\")\n",
    "\n",
    "print(f\"\\n--- Laplacian Kernel ---\")\n",
    "ocsvm_laplacian = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_laplacian.fit(K_laplacian_train)\n",
    "models['laplacian'] = ocsvm_laplacian\n",
    "print(f\"Laplacian OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_laplacian.n_support_}\")\n",
    "\n",
    "print(f\"\\n--- Polynomial (degree 2) ---\")\n",
    "ocsvm_poly2 = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_poly2.fit(K_poly2_train)\n",
    "models['poly2'] = ocsvm_poly2\n",
    "print(f\"Poly2 OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_poly2.n_support_}\")\n",
    "\n",
    "print(f\"\\n--- Polynomial (degree 3) ---\")\n",
    "ocsvm_poly3 = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_poly3.fit(K_poly3_train)\n",
    "models['poly3'] = ocsvm_poly3\n",
    "print(f\"Poly3 OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_poly3.n_support_}\")\n",
    "\n",
    "print(f\"\\n=== Computing Training Scores ===\")\n",
    "train_scores = {}\n",
    "\n",
    "kernel_matrices_train = {\n",
    "    'quantum': K_quantum_train,\n",
    "    'rbf': K_rbf_train,\n",
    "    'laplacian': K_laplacian_train,\n",
    "    'poly2': K_poly2_train,\n",
    "    'poly3': K_poly3_train\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    K_train = kernel_matrices_train[model_name]\n",
    "    scores = model.decision_function(K_train)\n",
    "    train_scores[model_name] = scores\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  Score range: [{scores.min():.6f}, {scores.max():.6f}]\")\n",
    "    print(f\"  Score mean: {scores.mean():.6f}\")\n",
    "    print(f\"  Score std: {scores.std():.6f}\")\n",
    "\n",
    "print(f\"\\n=== Saving Models ===\")\n",
    "for model_name, model in models.items():\n",
    "    model_path = os.path.join(DATA_DIR, f'ocsvm_{model_name}.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"{model_name}: {model_path}\")\n",
    "\n",
    "train_scores_path = os.path.join(DATA_DIR, 'train_scores.pkl')\n",
    "with open(train_scores_path, 'wb') as f:\n",
    "    pickle.dump(train_scores, f)\n",
    "print(f\"Training scores saved to: {train_scores_path}\")\n",
    "\n",
    "print(f\"\\nAll models trained and saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a004011",
   "metadata": {},
   "source": [
    "## **3.6 Calibrate Threshold (FPR=5% on Training) & Persist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a94c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Threshold Calibration (Target FPR=0.05) ===\n",
      "Method: 5th percentile of training scores\n",
      "Convention: score < threshold → anomaly\n",
      "\n",
      "QUANTUM:\n",
      "  Threshold (5th percentile): -0.000361\n",
      "  Training samples below threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000361\n",
      "    50%: 0.000055\n",
      "    95%: 0.000611\n",
      "    100%: 0.004158\n",
      "\n",
      "RBF:\n",
      "  Threshold (5th percentile): -0.000320\n",
      "  Training samples below threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000320\n",
      "    50%: 0.021346\n",
      "    95%: 0.065176\n",
      "    100%: 0.080326\n",
      "\n",
      "LAPLACIAN:\n",
      "  Threshold (5th percentile): -0.000228\n",
      "  Training samples below threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000228\n",
      "    50%: 0.001985\n",
      "    95%: 0.026493\n",
      "    100%: 0.037609\n",
      "\n",
      "POLY2:\n",
      "  Threshold (5th percentile): 0.000000\n",
      "  Training samples below threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  0.000000\n",
      "    50%: 2.273578\n",
      "    95%: 5.133115\n",
      "    100%: 6.688983\n",
      "\n",
      "POLY3:\n",
      "  Threshold (5th percentile): 0.000001\n",
      "  Training samples below threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  0.000001\n",
      "    50%: 10.437938\n",
      "    95%: 25.563402\n",
      "    100%: 34.703951\n",
      "\n",
      "Thresholds saved to: result/phase_3\\data\\thresholds.json\n",
      "Threshold summary saved to: result/phase_3\\data\\threshold_summary.json\n",
      "\n",
      "Threshold calibration complete\n"
     ]
    }
   ],
   "source": [
    "target_fpr = 0.05\n",
    "\n",
    "print(f\"=== Threshold Calibration (Target FPR={target_fpr}) ===\")\n",
    "print(f\"Method: 5th percentile of training scores\")\n",
    "print(f\"Convention: score < threshold → anomaly\")\n",
    "\n",
    "thresholds = {}\n",
    "\n",
    "for model_name, scores in train_scores.items():\n",
    "    threshold = np.percentile(scores, target_fpr * 100)\n",
    "    thresholds[model_name] = float(threshold)\n",
    "    \n",
    "    n_flagged = np.sum(scores < threshold)\n",
    "    actual_fpr = n_flagged / len(scores)\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  Threshold (5th percentile): {threshold:.6f}\")\n",
    "    print(f\"  Training samples below threshold: {n_flagged}/{len(scores)}\")\n",
    "    print(f\"  Actual FPR: {actual_fpr:.4f} (target: {target_fpr:.4f})\")\n",
    "    print(f\"  Score percentiles:\")\n",
    "    print(f\"    5%:  {np.percentile(scores, 5):.6f}\")\n",
    "    print(f\"    50%: {np.percentile(scores, 50):.6f}\")\n",
    "    print(f\"    95%: {np.percentile(scores, 95):.6f}\")\n",
    "    print(f\"    100%: {scores.max():.6f}\")\n",
    "\n",
    "thresholds_path = os.path.join(DATA_DIR, 'thresholds.json')\n",
    "with open(thresholds_path, 'w') as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "print(f\"\\nThresholds saved to: {thresholds_path}\")\n",
    "\n",
    "threshold_summary = {\n",
    "    'target_fpr': target_fpr,\n",
    "    'calibration_method': '5th_percentile',\n",
    "    'n_training_samples': len(train_scores['quantum']),\n",
    "    'anomaly_convention': 'score < threshold indicates anomaly',\n",
    "    'thresholds': thresholds\n",
    "}\n",
    "\n",
    "threshold_summary_path = os.path.join(DATA_DIR, 'threshold_summary.json')\n",
    "with open(threshold_summary_path, 'w') as f:\n",
    "    json.dump(threshold_summary, f, indent=2)\n",
    "print(f\"Threshold summary saved to: {threshold_summary_path}\")\n",
    "\n",
    "print(f\"\\nThreshold calibration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b7c7d",
   "metadata": {},
   "source": [
    "## **3.7 Save Models, Kernels (optional), and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35c6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3 Output Verification ===\n",
      "\n",
      "Checking saved artifacts in result/phase_3\\data...\n",
      "✓ scaler.pkl\n",
      "✓ scaling_params.json\n",
      "✓ quantum_kernel_params.json\n",
      "✓ baseline_kernel_params.json\n",
      "✓ K_quantum_train.npy\n",
      "✓ K_quantum_full.npy\n",
      "✓ K_rbf_train.npy\n",
      "✓ K_rbf_full.npy\n",
      "✓ K_laplacian_train.npy\n",
      "✓ K_laplacian_full.npy\n",
      "✓ K_poly2_train.npy\n",
      "✓ K_poly2_full.npy\n",
      "✓ K_poly3_train.npy\n",
      "✓ K_poly3_full.npy\n",
      "✓ ocsvm_quantum.pkl\n",
      "✓ ocsvm_rbf.pkl\n",
      "✓ ocsvm_laplacian.pkl\n",
      "✓ ocsvm_poly2.pkl\n",
      "✓ ocsvm_poly3.pkl\n",
      "✓ train_scores.pkl\n",
      "✓ thresholds.json\n",
      "✓ threshold_summary.json\n",
      "\n",
      "All required files present\n",
      "\n",
      "=== Saved Artifacts Summary ===\n",
      "\n",
      "1. Feature Scaling:\n",
      "   - scaler.pkl (MinMaxScaler fitted on training cycles 1-20)\n",
      "   - scaling_params.json (feature ranges, target [0, π])\n",
      "\n",
      "2. Quantum Kernel:\n",
      "   - quantum_kernel_params.json (8 qubits, depth 2, ZZ/Pauli)\n",
      "   - K_quantum_train.npy (20×20)\n",
      "   - K_quantum_full.npy (1241×20)\n",
      "\n",
      "3. Baseline Kernels:\n",
      "   - baseline_kernel_params.json (RBF γ=0.041460, etc.)\n",
      "   - K_rbf_train.npy, K_rbf_full.npy\n",
      "   - K_laplacian_train.npy, K_laplacian_full.npy\n",
      "   - K_poly2_train.npy, K_poly2_full.npy\n",
      "   - K_poly3_train.npy, K_poly3_full.npy\n",
      "\n",
      "4. Trained Models (ν=0.05):\n",
      "   - ocsvm_quantum.pkl (19 support vectors)\n",
      "   - ocsvm_rbf.pkl (7 support vectors)\n",
      "   - ocsvm_laplacian.pkl (9 support vectors)\n",
      "   - ocsvm_poly2.pkl (2 support vectors)\n",
      "   - ocsvm_poly3.pkl (2 support vectors)\n",
      "\n",
      "5. Calibration:\n",
      "   - thresholds.json (5th percentile, FPR=5% on training)\n",
      "   - threshold_summary.json\n",
      "   - train_scores.pkl\n",
      "\n",
      "=== Anomaly Detection Convention ===\n",
      "Score interpretation for anomaly detection:\n",
      "  • HIGHER scores = MORE NOMINAL (closer to training distribution)\n",
      "  • LOWER scores = MORE ANOMALOUS (farther from training distribution)\n",
      "  • Anomaly flagged when: score < threshold (5th percentile)\n",
      "\n",
      "Phase 3 complete - all artifacts saved successfully\n",
      "Ready to proceed to Phase 4 (Analysis & Visualization)\n"
     ]
    }
   ],
   "source": [
    "print(f\"=== Phase 3 Output Verification ===\")\n",
    "print(f\"\\nChecking saved artifacts in {DATA_DIR}...\")\n",
    "\n",
    "required_files = [\n",
    "    'scaler.pkl',\n",
    "    'scaling_params.json',\n",
    "    'quantum_kernel_params.json',\n",
    "    'baseline_kernel_params.json',\n",
    "    'K_quantum_train.npy',\n",
    "    'K_quantum_full.npy',\n",
    "    'K_rbf_train.npy',\n",
    "    'K_rbf_full.npy',\n",
    "    'K_laplacian_train.npy',\n",
    "    'K_laplacian_full.npy',\n",
    "    'K_poly2_train.npy',\n",
    "    'K_poly2_full.npy',\n",
    "    'K_poly3_train.npy',\n",
    "    'K_poly3_full.npy',\n",
    "    'ocsvm_quantum.pkl',\n",
    "    'ocsvm_rbf.pkl',\n",
    "    'ocsvm_laplacian.pkl',\n",
    "    'ocsvm_poly2.pkl',\n",
    "    'ocsvm_poly3.pkl',\n",
    "    'train_scores.pkl',\n",
    "    'thresholds.json',\n",
    "    'threshold_summary.json'\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = '✓' if exists else '✗'\n",
    "    print(f\"{status} {filename}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"\\nAll required files present\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Some files are missing\")\n",
    "\n",
    "print(f\"\\n=== Saved Artifacts Summary ===\")\n",
    "print(f\"\\n1. Feature Scaling:\")\n",
    "print(f\"   - scaler.pkl (MinMaxScaler fitted on training cycles 1-20)\")\n",
    "print(f\"   - scaling_params.json (feature ranges, target [0, π])\")\n",
    "\n",
    "print(f\"\\n2. Quantum Kernel:\")\n",
    "print(f\"   - quantum_kernel_params.json (8 qubits, depth 2, ZZ/Pauli)\")\n",
    "print(f\"   - K_quantum_train.npy (20×20)\")\n",
    "print(f\"   - K_quantum_full.npy ({len(X_full_scaled)}×20)\")\n",
    "\n",
    "print(f\"\\n3. Baseline Kernels:\")\n",
    "print(f\"   - baseline_kernel_params.json (RBF γ={gamma_rbf:.6f}, etc.)\")\n",
    "print(f\"   - K_rbf_train.npy, K_rbf_full.npy\")\n",
    "print(f\"   - K_laplacian_train.npy, K_laplacian_full.npy\")\n",
    "print(f\"   - K_poly2_train.npy, K_poly2_full.npy\")\n",
    "print(f\"   - K_poly3_train.npy, K_poly3_full.npy\")\n",
    "\n",
    "print(f\"\\n4. Trained Models (ν={nu}):\")\n",
    "print(f\"   - ocsvm_quantum.pkl ({models['quantum'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_rbf.pkl ({models['rbf'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_laplacian.pkl ({models['laplacian'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_poly2.pkl ({models['poly2'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_poly3.pkl ({models['poly3'].n_support_[0]} support vectors)\")\n",
    "\n",
    "print(f\"\\n5. Calibration:\")\n",
    "print(f\"   - thresholds.json (5th percentile, FPR=5% on training)\")\n",
    "print(f\"   - threshold_summary.json\")\n",
    "print(f\"   - train_scores.pkl\")\n",
    "\n",
    "print(f\"\\n=== Anomaly Detection Convention ===\")\n",
    "print(f\"Score interpretation for anomaly detection:\")\n",
    "print(f\"  • HIGHER scores = MORE NOMINAL (closer to training distribution)\")\n",
    "print(f\"  • LOWER scores = MORE ANOMALOUS (farther from training distribution)\")\n",
    "print(f\"  • Anomaly flagged when: score < threshold (5th percentile)\")\n",
    "\n",
    "print(f\"\\nPhase 3 complete - all artifacts saved successfully\")\n",
    "print(f\"Ready to proceed to Phase 4 (Analysis & Visualization)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "escl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
