{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324bf0d6",
   "metadata": {},
   "source": [
    "## **3.1 Load Features & Select Training Cycles (1..N, N=20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018cd799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded features from: result/phase_2/data/features.csv\n",
      "  Shape: (1241, 9)\n",
      "  Total cycles: 1241\n",
      "\n",
      "=== Training Configuration ===\n",
      "Training cycles: 1 to 20 (inclusive)\n",
      "Training samples: 20\n",
      "Test samples: 1221\n",
      "\n",
      "✓ Training subset selected\n",
      "  Cycle range: [1, 20]\n",
      "  Number of samples: 20\n",
      "\n",
      "✓ Feature columns (n=8): ['capacity_Ah', 'energy_Wh', 'duration_s', 'v_min', 'v_max', 'v_mean', 'i_rms', 'dVdt_abs_mean']\n",
      "\n",
      "=== Training Data Summary ===\n",
      "       cycle_idx  capacity_Ah  energy_Wh  duration_s      v_min      v_max  \\\n",
      "count   20.00000    20.000000  20.000000    20.00000  20.000000  20.000000   \n",
      "mean    10.50000     3.248586  11.613322  3655.70000   2.999985   4.083179   \n",
      "std      5.91608     0.011178   0.043639    12.57441   0.000046   0.000646   \n",
      "min      1.00000     3.234622  11.556919  3640.00000   2.999850   4.081560   \n",
      "25%      5.75000     3.239967  11.579369  3646.00000   3.000000   4.083050   \n",
      "50%     10.50000     3.246181  11.604447  3653.00000   3.000000   4.083240   \n",
      "75%     15.25000     3.253749  11.635044  3661.50000   3.000000   4.083700   \n",
      "max     20.00000     3.270184  11.696697  3680.00000   3.000000   4.083850   \n",
      "\n",
      "          v_mean      i_rms  dVdt_abs_mean  \n",
      "count  20.000000  20.000000      20.000000  \n",
      "mean    3.574754   3.199526       0.000363  \n",
      "std     0.001201   0.000007       0.000002  \n",
      "min     3.572743   3.199514       0.000359  \n",
      "25%     3.573824   3.199521       0.000361  \n",
      "50%     3.574794   3.199528       0.000363  \n",
      "75%     3.575761   3.199531       0.000364  \n",
      "max     3.576643   3.199535       0.000366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.quantum_info import Statevector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create Phase 3 output directories\n",
    "os.makedirs('result/phase_3/data', exist_ok=True)\n",
    "\n",
    "# Load features\n",
    "features_df = pd.read_csv('result/phase_2/data/features.csv')\n",
    "print(f\"✓ Loaded features from: result/phase_2/data/features.csv\")\n",
    "print(f\"  Shape: {features_df.shape}\")\n",
    "print(f\"  Total cycles: {len(features_df)}\")\n",
    "\n",
    "# Set training parameter\n",
    "N = 20  # Number of nominal training cycles\n",
    "print(f\"\\n=== Training Configuration ===\")\n",
    "print(f\"Training cycles: 1 to {N} (inclusive)\")\n",
    "print(f\"Training samples: {N}\")\n",
    "print(f\"Test samples: {len(features_df) - N}\")\n",
    "\n",
    "# Select training subset (cycles 1 to N)\n",
    "train_mask = (features_df['cycle_idx'] >= 1) & (features_df['cycle_idx'] <= N)\n",
    "train_df = features_df[train_mask].copy()\n",
    "print(f\"\\n✓ Training subset selected\")\n",
    "print(f\"  Cycle range: [{train_df['cycle_idx'].min()}, {train_df['cycle_idx'].max()}]\")\n",
    "print(f\"  Number of samples: {len(train_df)}\")\n",
    "\n",
    "# Extract feature columns (8 features)\n",
    "feature_cols = ['capacity_Ah', 'energy_Wh', 'duration_s', \n",
    "                'v_min', 'v_max', 'v_mean', 'i_rms', 'dVdt_abs_mean']\n",
    "print(f\"\\n✓ Feature columns (n={len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Display training data summary\n",
    "print(f\"\\n=== Training Data Summary ===\")\n",
    "print(train_df[['cycle_idx'] + feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b679fa",
   "metadata": {},
   "source": [
    "## **3.2 Scale Features to [0, π] Using Training Min–Max (Persist Scaler)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7eb6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Scaling to [0, π] ===\n",
      "Training feature matrix shape: (20, 8)\n",
      "Full feature matrix shape: (1241, 8)\n",
      "\n",
      "--- Training Set Min/Max (per feature) ---\n",
      "capacity_Ah         : [3.234622, 3.270184]\n",
      "energy_Wh           : [11.556919, 11.696697]\n",
      "duration_s          : [3640.000000, 3680.000000]\n",
      "v_min               : [2.999850, 3.000000]\n",
      "v_max               : [4.081560, 4.083850]\n",
      "v_mean              : [3.572743, 3.576643]\n",
      "i_rms               : [3.199514, 3.199535]\n",
      "dVdt_abs_mean       : [0.000359, 0.000366]\n",
      "\n",
      "✓ Features scaled to [0, π]\n",
      "  Training scaled shape: (20, 8)\n",
      "  Full scaled shape: (1241, 8)\n",
      "\n",
      "--- Scaled Training Data Range Check ---\n",
      "  Min values: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  Max values: [3.14159265 3.14159265 3.14159265 3.14159265 3.14159265 3.14159265\n",
      " 3.14159265 3.14159265]\n",
      "  Expected: all values in [0, 3.141593]\n",
      "\n",
      "✓ Scaler saved to: result/phase_3/data/scaler.pkl\n",
      "✓ Scaling parameters saved to: result/phase_3/data/scaling_params.json\n"
     ]
    }
   ],
   "source": [
    "# Extract feature matrices\n",
    "X_train = train_df[feature_cols].values\n",
    "X_full = features_df[feature_cols].values\n",
    "\n",
    "print(f\"=== Feature Scaling to [0, π] ===\")\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Full feature matrix shape: {X_full.shape}\")\n",
    "\n",
    "# Compute min-max bounds from training data only\n",
    "train_min = X_train.min(axis=0)\n",
    "train_max = X_train.max(axis=0)\n",
    "\n",
    "print(f\"\\n--- Training Set Min/Max (per feature) ---\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"{col:20s}: [{train_min[i]:.6f}, {train_max[i]:.6f}]\")\n",
    "\n",
    "# Create sklearn MinMaxScaler fitted to training data\n",
    "# Scale from training range to [0, π]\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both training and full datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_full_scaled = scaler.transform(X_full)\n",
    "\n",
    "print(f\"\\n✓ Features scaled to [0, π]\")\n",
    "print(f\"  Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Full scaled shape: {X_full_scaled.shape}\")\n",
    "\n",
    "# Verify scaling range\n",
    "print(f\"\\n--- Scaled Training Data Range Check ---\")\n",
    "print(f\"  Min values: {X_train_scaled.min(axis=0)}\")\n",
    "print(f\"  Max values: {X_train_scaled.max(axis=0)}\")\n",
    "print(f\"  Expected: all values in [0, {np.pi:.6f}]\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = 'result/phase_3/data/scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\n✓ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save scaling parameters for documentation\n",
    "scaling_params = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'train_min': train_min.tolist(),\n",
    "    'train_max': train_max.tolist(),\n",
    "    'target_range': [0, np.pi],\n",
    "    'n_features': len(feature_cols)\n",
    "}\n",
    "scaling_params_path = 'result/phase_3/data/scaling_params.json'\n",
    "with open(scaling_params_path, 'w') as f:\n",
    "    json.dump(scaling_params, f, indent=2)\n",
    "print(f\"✓ Scaling parameters saved to: {scaling_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dfd0a",
   "metadata": {},
   "source": [
    "## **3.3 Build Quantum Feature Map (ZZ/Pauli, Depth 1–2, 8 Qubits) & Kernel (Precomputed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4478b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quantum Feature Map Construction ===\n",
      "Number of qubits: 8\n",
      "Entangling depth: 2\n",
      "Feature encoding: ZZ/Pauli (RZ + RY + ZZ gates)\n",
      "\n",
      "✓ Quantum circuit created\n",
      "  Circuit depth: 52\n",
      "  Number of gates: 80\n",
      "  Parameters: 8\n",
      "\n",
      "=== Computing Quantum Kernel Matrices ===\n",
      "This may take several minutes...\n",
      "\n",
      "--- Training kernel K_train (20 × 20) ---\n",
      "  Progress: 20/20 rows computed\n",
      "\n",
      "--- Full kernel K_full (1241 × 20) for scoring ---\n",
      "  Progress: 50/1241 rows computed\n",
      "  Progress: 100/1241 rows computed\n",
      "  Progress: 150/1241 rows computed\n",
      "  Progress: 200/1241 rows computed\n",
      "  Progress: 250/1241 rows computed\n",
      "  Progress: 300/1241 rows computed\n",
      "  Progress: 350/1241 rows computed\n",
      "  Progress: 400/1241 rows computed\n",
      "  Progress: 450/1241 rows computed\n",
      "  Progress: 500/1241 rows computed\n",
      "  Progress: 550/1241 rows computed\n",
      "  Progress: 600/1241 rows computed\n",
      "  Progress: 650/1241 rows computed\n",
      "  Progress: 700/1241 rows computed\n",
      "  Progress: 750/1241 rows computed\n",
      "  Progress: 800/1241 rows computed\n",
      "  Progress: 850/1241 rows computed\n",
      "  Progress: 900/1241 rows computed\n",
      "  Progress: 950/1241 rows computed\n",
      "  Progress: 1000/1241 rows computed\n",
      "  Progress: 1050/1241 rows computed\n",
      "  Progress: 1100/1241 rows computed\n",
      "  Progress: 1150/1241 rows computed\n",
      "  Progress: 1200/1241 rows computed\n",
      "  Progress: 1241/1241 rows computed\n",
      "\n",
      "✓ Quantum kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "  K_train diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "\n",
      "✓ Kernel matrices saved\n",
      "  result/phase_3/data/K_quantum_train.npy\n",
      "  result/phase_3/data/K_quantum_full.npy\n",
      "✓ Quantum parameters saved to: result/phase_3/data/quantum_kernel_params.json\n"
     ]
    }
   ],
   "source": [
    "def create_quantum_feature_map(n_qubits=8, depth=2):\n",
    "    \"\"\"\n",
    "    Create a ZZ/Pauli entangling feature map circuit.\n",
    "    \n",
    "    Args:\n",
    "        n_qubits: Number of qubits (must match feature dimension)\n",
    "        depth: Number of entangling layers (1 or 2)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit with parameter vector\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    params = ParameterVector('x', n_qubits)\n",
    "    \n",
    "    for d in range(depth):\n",
    "        # Pauli rotation layer (RZ, RY)\n",
    "        for i in range(n_qubits):\n",
    "            qc.rz(params[i], i)\n",
    "            qc.ry(params[i], i)\n",
    "        \n",
    "        # ZZ entangling layer (nearest-neighbor + wrap-around)\n",
    "        for i in range(n_qubits):\n",
    "            j = (i + 1) % n_qubits\n",
    "            # ZZ gate decomposition: CNOT - RZ - CNOT\n",
    "            qc.cx(i, j)\n",
    "            qc.rz(2 * (params[i] - params[j]), j)\n",
    "            qc.cx(i, j)\n",
    "    \n",
    "    return qc, params\n",
    "\n",
    "def quantum_kernel_element(x1, x2, feature_map, params):\n",
    "    \"\"\"\n",
    "    Compute quantum kernel element K(x1, x2) = |⟨φ(x1)|φ(x2)⟩|²\n",
    "    \n",
    "    Args:\n",
    "        x1, x2: Feature vectors (scaled to [0, π])\n",
    "        feature_map: Quantum circuit\n",
    "        params: Parameter vector\n",
    "    \n",
    "    Returns:\n",
    "        Kernel value (float)\n",
    "    \"\"\"\n",
    "    # Bind parameters for x1\n",
    "    qc1 = feature_map.assign_parameters({params[i]: x1[i] for i in range(len(x1))})\n",
    "    state1 = Statevector.from_instruction(qc1)\n",
    "    \n",
    "    # Bind parameters for x2\n",
    "    qc2 = feature_map.assign_parameters({params[i]: x2[i] for i in range(len(x2))})\n",
    "    state2 = Statevector.from_instruction(qc2)\n",
    "    \n",
    "    # Compute inner product |⟨ψ1|ψ2⟩|²\n",
    "    overlap = np.abs(state1.inner(state2)) ** 2\n",
    "    return overlap\n",
    "\n",
    "def compute_quantum_kernel_matrix(X1, X2, feature_map, params):\n",
    "    \"\"\"\n",
    "    Compute full quantum kernel matrix K[i,j] = K(X1[i], X2[j])\n",
    "    \n",
    "    Args:\n",
    "        X1: First set of samples (n_samples_1, n_features)\n",
    "        X2: Second set of samples (n_samples_2, n_features)\n",
    "        feature_map: Quantum circuit\n",
    "        params: Parameter vector\n",
    "    \n",
    "    Returns:\n",
    "        Kernel matrix (n_samples_1, n_samples_2)\n",
    "    \"\"\"\n",
    "    n1, n2 = len(X1), len(X2)\n",
    "    K = np.zeros((n1, n2))\n",
    "    \n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = quantum_kernel_element(X1[i], X2[j], feature_map, params)\n",
    "        \n",
    "        if (i + 1) % 50 == 0 or i == n1 - 1:\n",
    "            print(f\"  Progress: {i+1}/{n1} rows computed\")\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Create quantum feature map\n",
    "n_qubits = 8  # Match feature dimension\n",
    "depth = 2\n",
    "print(f\"=== Quantum Feature Map Construction ===\")\n",
    "print(f\"Number of qubits: {n_qubits}\")\n",
    "print(f\"Entangling depth: {depth}\")\n",
    "print(f\"Feature encoding: ZZ/Pauli (RZ + RY + ZZ gates)\")\n",
    "\n",
    "feature_map, params = create_quantum_feature_map(n_qubits=n_qubits, depth=depth)\n",
    "print(f\"\\n✓ Quantum circuit created\")\n",
    "print(f\"  Circuit depth: {feature_map.depth()}\")\n",
    "print(f\"  Number of gates: {len(feature_map.data)}\")\n",
    "print(f\"  Parameters: {len(params)}\")\n",
    "\n",
    "# Compute quantum kernel matrices\n",
    "print(f\"\\n=== Computing Quantum Kernel Matrices ===\")\n",
    "print(f\"This may take several minutes...\")\n",
    "\n",
    "print(f\"\\n--- Training kernel K_train (20 × 20) ---\")\n",
    "K_quantum_train = compute_quantum_kernel_matrix(X_train_scaled, X_train_scaled, \n",
    "                                                 feature_map, params)\n",
    "\n",
    "print(f\"\\n--- Full kernel K_full (1241 × 20) for scoring ---\")\n",
    "K_quantum_full = compute_quantum_kernel_matrix(X_full_scaled, X_train_scaled, \n",
    "                                                feature_map, params)\n",
    "\n",
    "print(f\"\\n✓ Quantum kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_quantum_train.shape}\")\n",
    "print(f\"  K_full shape: {K_quantum_full.shape}\")\n",
    "print(f\"  K_train diagonal (should be ~1.0): {np.diag(K_quantum_train)[:5]}\")\n",
    "\n",
    "# Save quantum kernel matrices\n",
    "np.save('result/phase_3/data/K_quantum_train.npy', K_quantum_train)\n",
    "np.save('result/phase_3/data/K_quantum_full.npy', K_quantum_full)\n",
    "print(f\"\\n✓ Kernel matrices saved\")\n",
    "print(f\"  result/phase_3/data/K_quantum_train.npy\")\n",
    "print(f\"  result/phase_3/data/K_quantum_full.npy\")\n",
    "\n",
    "# Save quantum kernel parameters\n",
    "quantum_params = {\n",
    "    'type': 'ZZ_Pauli_entangling',\n",
    "    'n_qubits': n_qubits,\n",
    "    'depth': depth,\n",
    "    'circuit_depth': feature_map.depth(),\n",
    "    'n_gates': len(feature_map.data),\n",
    "    'encoding': 'RZ + RY rotations with ZZ entanglement'\n",
    "}\n",
    "quantum_params_path = 'result/phase_3/data/quantum_kernel_params.json'\n",
    "with open(quantum_params_path, 'w') as f:\n",
    "    json.dump(quantum_params, f, indent=2)\n",
    "print(f\"✓ Quantum parameters saved to: {quantum_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8693fd",
   "metadata": {},
   "source": [
    "## **3.4 Prepare Baseline Kernels (RBF γ=median heuristic; Laplacian; Poly deg 2–3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592e0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RBF Kernel (Median Heuristic) ===\n",
      "Training pairwise distances:\n",
      "  Median distance: 3.472745\n",
      "  Mean distance: 3.537774\n",
      "  Std distance: 1.446672\n",
      "  Range: [0.766265, 6.955260]\n",
      "\n",
      "✓ RBF gamma (median heuristic): 0.041460\n",
      "\n",
      "✓ RBF kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "  K_train diagonal: [1. 1. 1. 1. 1.]\n",
      "\n",
      "=== Laplacian Kernel ===\n",
      "Using same gamma: 0.041460\n",
      "✓ Laplacian kernel matrices computed\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "\n",
      "=== Polynomial Kernels ===\n",
      "Degree 2:\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "Degree 3:\n",
      "  K_train shape: (20, 20)\n",
      "  K_full shape: (1241, 20)\n",
      "\n",
      "=== Saving Baseline Kernel Matrices ===\n",
      "✓ All baseline kernel matrices saved\n",
      "✓ Baseline parameters saved to: result/phase_3/data/baseline_kernel_params.json\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise distances on training set for median heuristic\n",
    "train_distances = pairwise_distances(X_train_scaled, metric='euclidean')\n",
    "# Extract upper triangle (exclude diagonal)\n",
    "train_dist_flat = train_distances[np.triu_indices_from(train_distances, k=1)]\n",
    "\n",
    "# Median heuristic: γ = 1 / (2 * median²)\n",
    "median_dist = np.median(train_dist_flat)\n",
    "gamma_rbf = 1.0 / (2 * median_dist ** 2)\n",
    "\n",
    "print(f\"=== RBF Kernel (Median Heuristic) ===\")\n",
    "print(f\"Training pairwise distances:\")\n",
    "print(f\"  Median distance: {median_dist:.6f}\")\n",
    "print(f\"  Mean distance: {train_dist_flat.mean():.6f}\")\n",
    "print(f\"  Std distance: {train_dist_flat.std():.6f}\")\n",
    "print(f\"  Range: [{train_dist_flat.min():.6f}, {train_dist_flat.max():.6f}]\")\n",
    "print(f\"\\n✓ RBF gamma (median heuristic): {gamma_rbf:.6f}\")\n",
    "\n",
    "# Compute RBF kernel matrices\n",
    "from sklearn.metrics.pairwise import rbf_kernel, laplacian_kernel, polynomial_kernel\n",
    "\n",
    "K_rbf_train = rbf_kernel(X_train_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "K_rbf_full = rbf_kernel(X_full_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "\n",
    "print(f\"\\n✓ RBF kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_rbf_train.shape}\")\n",
    "print(f\"  K_full shape: {K_rbf_full.shape}\")\n",
    "print(f\"  K_train diagonal: {np.diag(K_rbf_train)[:5]}\")\n",
    "\n",
    "# Laplacian kernel (same gamma)\n",
    "print(f\"\\n=== Laplacian Kernel ===\")\n",
    "print(f\"Using same gamma: {gamma_rbf:.6f}\")\n",
    "\n",
    "K_laplacian_train = laplacian_kernel(X_train_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "K_laplacian_full = laplacian_kernel(X_full_scaled, X_train_scaled, gamma=gamma_rbf)\n",
    "\n",
    "print(f\"✓ Laplacian kernel matrices computed\")\n",
    "print(f\"  K_train shape: {K_laplacian_train.shape}\")\n",
    "print(f\"  K_full shape: {K_laplacian_full.shape}\")\n",
    "\n",
    "# Polynomial kernels (degree 2 and 3)\n",
    "print(f\"\\n=== Polynomial Kernels ===\")\n",
    "\n",
    "# Degree 2\n",
    "K_poly2_train = polynomial_kernel(X_train_scaled, X_train_scaled, degree=2, coef0=1)\n",
    "K_poly2_full = polynomial_kernel(X_full_scaled, X_train_scaled, degree=2, coef0=1)\n",
    "\n",
    "print(f\"Degree 2:\")\n",
    "print(f\"  K_train shape: {K_poly2_train.shape}\")\n",
    "print(f\"  K_full shape: {K_poly2_full.shape}\")\n",
    "\n",
    "# Degree 3\n",
    "K_poly3_train = polynomial_kernel(X_train_scaled, X_train_scaled, degree=3, coef0=1)\n",
    "K_poly3_full = polynomial_kernel(X_full_scaled, X_train_scaled, degree=3, coef0=1)\n",
    "\n",
    "print(f\"Degree 3:\")\n",
    "print(f\"  K_train shape: {K_poly3_train.shape}\")\n",
    "print(f\"  K_full shape: {K_poly3_full.shape}\")\n",
    "\n",
    "# Save all baseline kernel matrices\n",
    "print(f\"\\n=== Saving Baseline Kernel Matrices ===\")\n",
    "np.save('result/phase_3/data/K_rbf_train.npy', K_rbf_train)\n",
    "np.save('result/phase_3/data/K_rbf_full.npy', K_rbf_full)\n",
    "np.save('result/phase_3/data/K_laplacian_train.npy', K_laplacian_train)\n",
    "np.save('result/phase_3/data/K_laplacian_full.npy', K_laplacian_full)\n",
    "np.save('result/phase_3/data/K_poly2_train.npy', K_poly2_train)\n",
    "np.save('result/phase_3/data/K_poly2_full.npy', K_poly2_full)\n",
    "np.save('result/phase_3/data/K_poly3_train.npy', K_poly3_train)\n",
    "np.save('result/phase_3/data/K_poly3_full.npy', K_poly3_full)\n",
    "\n",
    "print(f\"✓ All baseline kernel matrices saved\")\n",
    "\n",
    "# Save baseline kernel parameters\n",
    "baseline_params = {\n",
    "    'rbf': {\n",
    "        'type': 'radial_basis_function',\n",
    "        'gamma': gamma_rbf,\n",
    "        'gamma_method': 'median_heuristic',\n",
    "        'median_distance': median_dist\n",
    "    },\n",
    "    'laplacian': {\n",
    "        'type': 'laplacian',\n",
    "        'gamma': gamma_rbf,\n",
    "        'gamma_method': 'same_as_rbf'\n",
    "    },\n",
    "    'polynomial_deg2': {\n",
    "        'type': 'polynomial',\n",
    "        'degree': 2,\n",
    "        'coef0': 1\n",
    "    },\n",
    "    'polynomial_deg3': {\n",
    "        'type': 'polynomial',\n",
    "        'degree': 3,\n",
    "        'coef0': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "baseline_params_path = 'result/phase_3/data/baseline_kernel_params.json'\n",
    "with open(baseline_params_path, 'w') as f:\n",
    "    json.dump(baseline_params, f, indent=2)\n",
    "print(f\"✓ Baseline parameters saved to: {baseline_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884db278",
   "metadata": {},
   "source": [
    "## **3.5 Train ν-OCSVM for Quantum & Baselines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fedd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ν-OCSVM Models ===\n",
      "ν parameter: 0.05 (target ~5% training outliers)\n",
      "Training samples: 20\n",
      "\n",
      "--- Quantum Kernel ---\n",
      "✓ Quantum OCSVM trained\n",
      "  Support vectors: [19]\n",
      "  Support vector indices: [0 1 2 3 4]... (showing first 5)\n",
      "\n",
      "--- RBF Kernel ---\n",
      "✓ RBF OCSVM trained\n",
      "  Support vectors: [7]\n",
      "\n",
      "--- Laplacian Kernel ---\n",
      "✓ Laplacian OCSVM trained\n",
      "  Support vectors: [9]\n",
      "\n",
      "--- Polynomial (degree 2) ---\n",
      "✓ Poly2 OCSVM trained\n",
      "  Support vectors: [2]\n",
      "\n",
      "--- Polynomial (degree 3) ---\n",
      "✓ Poly3 OCSVM trained\n",
      "  Support vectors: [2]\n",
      "\n",
      "=== Computing Training Scores ===\n",
      "\n",
      "QUANTUM:\n",
      "  Score range: [-0.000408, 0.004158]\n",
      "  Score mean: 0.000208\n",
      "  Score std: 0.000929\n",
      "\n",
      "RBF:\n",
      "  Score range: [-0.000320, 0.080326]\n",
      "  Score mean: 0.024584\n",
      "  Score std: 0.025870\n",
      "\n",
      "LAPLACIAN:\n",
      "  Score range: [-0.000428, 0.037609]\n",
      "  Score mean: 0.007758\n",
      "  Score std: 0.010630\n",
      "\n",
      "POLY2:\n",
      "  Score range: [0.000000, 6.688983]\n",
      "  Score mean: 2.490045\n",
      "  Score std: 1.748143\n",
      "\n",
      "POLY3:\n",
      "  Score range: [-0.000000, 34.703951]\n",
      "  Score mean: 12.005305\n",
      "  Score std: 8.940211\n",
      "\n",
      "=== Saving Models ===\n",
      "✓ quantum: result/phase_3/data/ocsvm_quantum.pkl\n",
      "✓ rbf: result/phase_3/data/ocsvm_rbf.pkl\n",
      "✓ laplacian: result/phase_3/data/ocsvm_laplacian.pkl\n",
      "✓ poly2: result/phase_3/data/ocsvm_poly2.pkl\n",
      "✓ poly3: result/phase_3/data/ocsvm_poly3.pkl\n",
      "✓ Training scores saved to: result/phase_3/data/train_scores.pkl\n",
      "\n",
      "✓ All models trained and saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Set nu parameter (target FPR on training data)\n",
    "nu = 0.05\n",
    "\n",
    "print(f\"=== Training ν-OCSVM Models ===\")\n",
    "print(f\"ν parameter: {nu} (target ~5% training outliers)\")\n",
    "print(f\"Training samples: {K_quantum_train.shape[0]}\")\n",
    "\n",
    "# Dictionary to store all models\n",
    "models = {}\n",
    "\n",
    "# Train Quantum OCSVM\n",
    "print(f\"\\n--- Quantum Kernel ---\")\n",
    "ocsvm_quantum = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_quantum.fit(K_quantum_train)\n",
    "models['quantum'] = ocsvm_quantum\n",
    "print(f\"✓ Quantum OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_quantum.n_support_}\")\n",
    "print(f\"  Support vector indices: {ocsvm_quantum.support_[:5]}... (showing first 5)\")\n",
    "\n",
    "# Train RBF OCSVM\n",
    "print(f\"\\n--- RBF Kernel ---\")\n",
    "ocsvm_rbf = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_rbf.fit(K_rbf_train)\n",
    "models['rbf'] = ocsvm_rbf\n",
    "print(f\"✓ RBF OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_rbf.n_support_}\")\n",
    "\n",
    "# Train Laplacian OCSVM\n",
    "print(f\"\\n--- Laplacian Kernel ---\")\n",
    "ocsvm_laplacian = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_laplacian.fit(K_laplacian_train)\n",
    "models['laplacian'] = ocsvm_laplacian\n",
    "print(f\"✓ Laplacian OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_laplacian.n_support_}\")\n",
    "\n",
    "# Train Polynomial degree 2 OCSVM\n",
    "print(f\"\\n--- Polynomial (degree 2) ---\")\n",
    "ocsvm_poly2 = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_poly2.fit(K_poly2_train)\n",
    "models['poly2'] = ocsvm_poly2\n",
    "print(f\"✓ Poly2 OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_poly2.n_support_}\")\n",
    "\n",
    "# Train Polynomial degree 3 OCSVM\n",
    "print(f\"\\n--- Polynomial (degree 3) ---\")\n",
    "ocsvm_poly3 = OneClassSVM(kernel='precomputed', nu=nu)\n",
    "ocsvm_poly3.fit(K_poly3_train)\n",
    "models['poly3'] = ocsvm_poly3\n",
    "print(f\"✓ Poly3 OCSVM trained\")\n",
    "print(f\"  Support vectors: {ocsvm_poly3.n_support_}\")\n",
    "\n",
    "# Compute decision scores on training data for all models\n",
    "print(f\"\\n=== Computing Training Scores ===\")\n",
    "train_scores = {}\n",
    "\n",
    "# Map kernel matrices to models\n",
    "kernel_matrices_train = {\n",
    "    'quantum': K_quantum_train,\n",
    "    'rbf': K_rbf_train,\n",
    "    'laplacian': K_laplacian_train,\n",
    "    'poly2': K_poly2_train,\n",
    "    'poly3': K_poly3_train\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    K_train = kernel_matrices_train[model_name]\n",
    "    scores = model.decision_function(K_train)\n",
    "    train_scores[model_name] = scores\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  Score range: [{scores.min():.6f}, {scores.max():.6f}]\")\n",
    "    print(f\"  Score mean: {scores.mean():.6f}\")\n",
    "    print(f\"  Score std: {scores.std():.6f}\")\n",
    "\n",
    "# Save all models\n",
    "print(f\"\\n=== Saving Models ===\")\n",
    "for model_name, model in models.items():\n",
    "    model_path = f'result/phase_3/data/ocsvm_{model_name}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"✓ {model_name}: {model_path}\")\n",
    "\n",
    "# Save training scores\n",
    "train_scores_path = 'result/phase_3/data/train_scores.pkl'\n",
    "with open(train_scores_path, 'wb') as f:\n",
    "    pickle.dump(train_scores, f)\n",
    "print(f\"✓ Training scores saved to: {train_scores_path}\")\n",
    "\n",
    "print(f\"\\n✓ All models trained and saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a004011",
   "metadata": {},
   "source": [
    "## **3.6 Calibrate Threshold (FPR=5% on Training) & Persist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a94c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Threshold Calibration (Target FPR=0.05) ===\n",
      "Method: 95th percentile of training scores\n",
      "  (scores below threshold = nominal)\n",
      "  (scores above threshold = anomaly)\n",
      "\n",
      "QUANTUM:\n",
      "  Threshold: 0.000611\n",
      "  Training samples above threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000361\n",
      "    50%: 0.000055\n",
      "    95%: 0.000611\n",
      "    100%: 0.004158\n",
      "\n",
      "RBF:\n",
      "  Threshold: 0.065176\n",
      "  Training samples above threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000320\n",
      "    50%: 0.021346\n",
      "    95%: 0.065176\n",
      "    100%: 0.080326\n",
      "\n",
      "LAPLACIAN:\n",
      "  Threshold: 0.026493\n",
      "  Training samples above threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  -0.000228\n",
      "    50%: 0.001985\n",
      "    95%: 0.026493\n",
      "    100%: 0.037609\n",
      "\n",
      "POLY2:\n",
      "  Threshold: 5.133115\n",
      "  Training samples above threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  0.000000\n",
      "    50%: 2.273578\n",
      "    95%: 5.133115\n",
      "    100%: 6.688983\n",
      "\n",
      "POLY3:\n",
      "  Threshold: 25.563402\n",
      "  Training samples above threshold: 1/20\n",
      "  Actual FPR: 0.0500 (target: 0.0500)\n",
      "  Score percentiles:\n",
      "    5%:  0.000001\n",
      "    50%: 10.437938\n",
      "    95%: 25.563402\n",
      "    100%: 34.703951\n",
      "\n",
      "✓ Thresholds saved to: result/phase_3/data/thresholds.json\n",
      "✓ Threshold summary saved to: result/phase_3/data/threshold_summary.json\n",
      "\n",
      "✓ Threshold calibration complete\n"
     ]
    }
   ],
   "source": [
    "# Calibrate thresholds to achieve 5% FPR on training data\n",
    "target_fpr = 0.05\n",
    "\n",
    "print(f\"=== Threshold Calibration (Target FPR={target_fpr}) ===\")\n",
    "print(f\"Method: 95th percentile of training scores\")\n",
    "print(f\"  (scores below threshold = nominal)\")\n",
    "print(f\"  (scores above threshold = anomaly)\")\n",
    "\n",
    "thresholds = {}\n",
    "\n",
    "for model_name, scores in train_scores.items():\n",
    "    # For OCSVM, negative scores typically indicate outliers\n",
    "    # But we use decision_function which can be positive or negative\n",
    "    # We want to set threshold such that 5% of training samples are flagged\n",
    "    # This means threshold at the 95th percentile\n",
    "    threshold = np.percentile(scores, 100 * (1 - target_fpr))\n",
    "    thresholds[model_name] = threshold\n",
    "    \n",
    "    # Count samples above threshold\n",
    "    n_flagged = np.sum(scores > threshold)\n",
    "    actual_fpr = n_flagged / len(scores)\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  Threshold: {threshold:.6f}\")\n",
    "    print(f\"  Training samples above threshold: {n_flagged}/{len(scores)}\")\n",
    "    print(f\"  Actual FPR: {actual_fpr:.4f} (target: {target_fpr:.4f})\")\n",
    "    print(f\"  Score percentiles:\")\n",
    "    print(f\"    5%:  {np.percentile(scores, 5):.6f}\")\n",
    "    print(f\"    50%: {np.percentile(scores, 50):.6f}\")\n",
    "    print(f\"    95%: {np.percentile(scores, 95):.6f}\")\n",
    "    print(f\"    100%: {np.percentile(scores, 100):.6f}\")\n",
    "\n",
    "# Save thresholds\n",
    "thresholds_path = 'result/phase_3/data/thresholds.json'\n",
    "with open(thresholds_path, 'w') as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "print(f\"\\n✓ Thresholds saved to: {thresholds_path}\")\n",
    "\n",
    "# Create threshold summary\n",
    "threshold_summary = {\n",
    "    'target_fpr': target_fpr,\n",
    "    'calibration_method': '95th_percentile',\n",
    "    'n_training_samples': len(train_scores['quantum']),\n",
    "    'thresholds': thresholds\n",
    "}\n",
    "\n",
    "threshold_summary_path = 'result/phase_3/data/threshold_summary.json'\n",
    "with open(threshold_summary_path, 'w') as f:\n",
    "    json.dump(threshold_summary, f, indent=2)\n",
    "print(f\"✓ Threshold summary saved to: {threshold_summary_path}\")\n",
    "\n",
    "print(f\"\\n✓ Threshold calibration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b7c7d",
   "metadata": {},
   "source": [
    "## **3.7 Save Models, Kernels (optional), and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35c6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3 Output Verification ===\n",
      "\n",
      "Checking saved artifacts in result/phase_3/data/...\n",
      "✓ scaler.pkl\n",
      "✓ scaling_params.json\n",
      "✓ quantum_kernel_params.json\n",
      "✓ baseline_kernel_params.json\n",
      "✓ K_quantum_train.npy\n",
      "✓ K_quantum_full.npy\n",
      "✓ K_rbf_train.npy\n",
      "✓ K_rbf_full.npy\n",
      "✓ K_laplacian_train.npy\n",
      "✓ K_laplacian_full.npy\n",
      "✓ K_poly2_train.npy\n",
      "✓ K_poly2_full.npy\n",
      "✓ K_poly3_train.npy\n",
      "✓ K_poly3_full.npy\n",
      "✓ ocsvm_quantum.pkl\n",
      "✓ ocsvm_rbf.pkl\n",
      "✓ ocsvm_laplacian.pkl\n",
      "✓ ocsvm_poly2.pkl\n",
      "✓ ocsvm_poly3.pkl\n",
      "✓ train_scores.pkl\n",
      "✓ thresholds.json\n",
      "✓ threshold_summary.json\n",
      "\n",
      "✓ All required files present\n",
      "\n",
      "=== Saved Artifacts Summary ===\n",
      "\n",
      "1. Feature Scaling:\n",
      "   - scaler.pkl (MinMaxScaler fitted on training cycles 1-20)\n",
      "   - scaling_params.json (feature ranges, target [0, π])\n",
      "\n",
      "2. Quantum Kernel:\n",
      "   - quantum_kernel_params.json (8 qubits, depth 2, ZZ/Pauli)\n",
      "   - K_quantum_train.npy (20×20)\n",
      "   - K_quantum_full.npy (1241×20)\n",
      "\n",
      "3. Baseline Kernels:\n",
      "   - baseline_kernel_params.json (RBF γ=0.041460, etc.)\n",
      "   - K_rbf_train.npy, K_rbf_full.npy\n",
      "   - K_laplacian_train.npy, K_laplacian_full.npy\n",
      "   - K_poly2_train.npy, K_poly2_full.npy\n",
      "   - K_poly3_train.npy, K_poly3_full.npy\n",
      "\n",
      "4. Trained Models (ν=0.05):\n",
      "   - ocsvm_quantum.pkl (19 support vectors)\n",
      "   - ocsvm_rbf.pkl (7 support vectors)\n",
      "   - ocsvm_laplacian.pkl (9 support vectors)\n",
      "   - ocsvm_poly2.pkl (2 support vectors)\n",
      "   - ocsvm_poly3.pkl (2 support vectors)\n",
      "\n",
      "5. Calibration:\n",
      "   - thresholds.json (95th percentile, FPR=5% on training)\n",
      "   - threshold_summary.json\n",
      "   - train_scores.pkl\n",
      "\n",
      "=== Anomaly Detection Convention ===\n",
      "IMPORTANT: Score interpretation for anomaly detection:\n",
      "  • HIGHER scores = MORE NOMINAL (closer to training distribution)\n",
      "  • LOWER scores = MORE ANOMALOUS (farther from training distribution)\n",
      "  • Anomaly flagged when: score > threshold (95th percentile)\n",
      "  • This is consistent with OCSVM decision_function behavior\n",
      "\n",
      "✓ Updated threshold_summary.json with convention clarification\n",
      "\n",
      "✓ Phase 3 artifacts saved successfully\n",
      "\n",
      "Total files saved: 22\n"
     ]
    }
   ],
   "source": [
    "# Verify all required files are saved\n",
    "print(f\"=== Phase 3 Output Verification ===\")\n",
    "print(f\"\\nChecking saved artifacts in result/phase_3/data/...\")\n",
    "\n",
    "required_files = [\n",
    "    'scaler.pkl',\n",
    "    'scaling_params.json',\n",
    "    'quantum_kernel_params.json',\n",
    "    'baseline_kernel_params.json',\n",
    "    'K_quantum_train.npy',\n",
    "    'K_quantum_full.npy',\n",
    "    'K_rbf_train.npy',\n",
    "    'K_rbf_full.npy',\n",
    "    'K_laplacian_train.npy',\n",
    "    'K_laplacian_full.npy',\n",
    "    'K_poly2_train.npy',\n",
    "    'K_poly2_full.npy',\n",
    "    'K_poly3_train.npy',\n",
    "    'K_poly3_full.npy',\n",
    "    'ocsvm_quantum.pkl',\n",
    "    'ocsvm_rbf.pkl',\n",
    "    'ocsvm_laplacian.pkl',\n",
    "    'ocsvm_poly2.pkl',\n",
    "    'ocsvm_poly3.pkl',\n",
    "    'train_scores.pkl',\n",
    "    'thresholds.json',\n",
    "    'threshold_summary.json'\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for filename in required_files:\n",
    "    filepath = f'result/phase_3/data/{filename}'\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = '✓' if exists else '✗'\n",
    "    print(f\"{status} {filename}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"\\n✓ All required files present\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: Some files are missing\")\n",
    "\n",
    "# Summary of saved artifacts\n",
    "print(f\"\\n=== Saved Artifacts Summary ===\")\n",
    "print(f\"\\n1. Feature Scaling:\")\n",
    "print(f\"   - scaler.pkl (MinMaxScaler fitted on training cycles 1-20)\")\n",
    "print(f\"   - scaling_params.json (feature ranges, target [0, π])\")\n",
    "\n",
    "print(f\"\\n2. Quantum Kernel:\")\n",
    "print(f\"   - quantum_kernel_params.json (8 qubits, depth 2, ZZ/Pauli)\")\n",
    "print(f\"   - K_quantum_train.npy (20×20)\")\n",
    "print(f\"   - K_quantum_full.npy (1241×20)\")\n",
    "\n",
    "print(f\"\\n3. Baseline Kernels:\")\n",
    "print(f\"   - baseline_kernel_params.json (RBF γ={gamma_rbf:.6f}, etc.)\")\n",
    "print(f\"   - K_rbf_train.npy, K_rbf_full.npy\")\n",
    "print(f\"   - K_laplacian_train.npy, K_laplacian_full.npy\")\n",
    "print(f\"   - K_poly2_train.npy, K_poly2_full.npy\")\n",
    "print(f\"   - K_poly3_train.npy, K_poly3_full.npy\")\n",
    "\n",
    "print(f\"\\n4. Trained Models (ν={nu}):\")\n",
    "print(f\"   - ocsvm_quantum.pkl ({models['quantum'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_rbf.pkl ({models['rbf'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_laplacian.pkl ({models['laplacian'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_poly2.pkl ({models['poly2'].n_support_[0]} support vectors)\")\n",
    "print(f\"   - ocsvm_poly3.pkl ({models['poly3'].n_support_[0]} support vectors)\")\n",
    "\n",
    "print(f\"\\n5. Calibration:\")\n",
    "print(f\"   - thresholds.json (95th percentile, FPR=5% on training)\")\n",
    "print(f\"   - threshold_summary.json\")\n",
    "print(f\"   - train_scores.pkl\")\n",
    "\n",
    "print(f\"\\n=== Anomaly Detection Convention ===\")\n",
    "print(f\"IMPORTANT: Score interpretation for anomaly detection:\")\n",
    "print(f\"  • HIGHER scores = MORE NOMINAL (closer to training distribution)\")\n",
    "print(f\"  • LOWER scores = MORE ANOMALOUS (farther from training distribution)\")\n",
    "print(f\"  • Anomaly flagged when: score > threshold (95th percentile)\")\n",
    "print(f\"  • This is consistent with OCSVM decision_function behavior\")\n",
    "\n",
    "# Update threshold summary with clarification\n",
    "threshold_summary_updated = {\n",
    "    'target_fpr': target_fpr,\n",
    "    'calibration_method': '95th_percentile',\n",
    "    'n_training_samples': len(train_scores['quantum']),\n",
    "    'anomaly_convention': 'score > threshold indicates anomaly',\n",
    "    'score_interpretation': 'higher score = more nominal, lower score = more anomalous',\n",
    "    'thresholds': thresholds,\n",
    "    'actual_fpr_per_model': {\n",
    "        model_name: float(np.sum(train_scores[model_name] > thresholds[model_name]) / len(train_scores[model_name]))\n",
    "        for model_name in thresholds.keys()\n",
    "    }\n",
    "}\n",
    "\n",
    "threshold_summary_path = 'result/phase_3/data/threshold_summary.json'\n",
    "with open(threshold_summary_path, 'w') as f:\n",
    "    json.dump(threshold_summary_updated, f, indent=2)\n",
    "print(f\"\\n✓ Updated threshold_summary.json with convention clarification\")\n",
    "\n",
    "print(f\"\\n✓ Phase 3 artifacts saved successfully\")\n",
    "print(f\"\\nTotal files saved: {len(required_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a7164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "escl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
